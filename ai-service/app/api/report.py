"""
Endpoints para an√°lise de relat√≥rios XP.
"""
from typing import Dict, Any  # ‚Üê ADICIONAR Dict
import json
from fastapi import APIRouter, HTTPException, BackgroundTasks 
from fastapi.responses import StreamingResponse

from app.models.requests import (    
    ReportAnalyzeAutoRequest, 
    ReportAnalyzePersonalizedRequest,
    ReportAnalyzeResponse,
    BatchReportRequest,
    BatchReportResponse,
    UltraBatchReportRequest, 
    UltraBatchReportResponse,
    GenerateUploadUrlsRequest,  # ‚Üê NOVO
    GenerateUploadUrlsResponse 
)
from app.workflows.report_workflow import create_report_analysis_workflow, create_report_analysis_workflow_from_data

from app.services.report_analyzer.batch_processing import process_batch_reports
from app.services.report_analyzer.ultra_batch_processing import process_ultra_batch_reports  
from app.services.report_analyzer.signed_url import generate_signed_url_for_upload  # ‚Üê NOVO

from pydantic import BaseModel
from typing import Dict, Any

from app.config import get_firestore_client, logger
from firebase_admin import firestore  
import uuid 



router = APIRouter(tags=["report"])  # Remover o prefix


@router.post("/analyze-auto", response_model=ReportAnalyzeResponse)
async def analyze_report_auto(request: ReportAnalyzeAutoRequest):
    """
    An√°lise autom√°tica completa (todos os dados).
    
    Processa o relat√≥rio XP e retorna:
    - Dados extra√≠dos
    - An√°lise de performance
    - Highlights e detractors
    - Mensagem WhatsApp formatada
    """
    try:
        state = {
            "file_content": request.file_content,
            "file_name": request.file_name,
            "user_id": request.user_id,
            "analysis_mode": "auto",
            "selected_fields": None
        }
        
        app = create_report_analysis_workflow()
        result = await app.ainvoke(state)
        
        return ReportAnalyzeResponse(
            success=True,
            extracted_data=result.get("extracted_data"),
            file_name=request.file_name, 
            highlights=result.get("highlights"),
            detractors=result.get("detractors"),
            final_message=result.get("final_message"),
            metadata=result.get("metadata"),
            error=result.get("error")
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/analyze-personalized", response_model=ReportAnalyzeResponse)
async def analyze_report_personalized(request: ReportAnalyzePersonalizedRequest):
    """
    An√°lise personalizada com campos selecionados pelo usu√°rio.
    
    Processa apenas os campos escolhidos pelo usu√°rio:
    - Campos top-level (monthlyReturn, yearlyReturn, etc.)
    - Classes de ativo espec√≠ficas
    - Highlights/detractors filtrados
    """
    try:
        state = {
            "file_content": request.file_content,
            "file_name": request.file_name,
            "user_id": request.user_id,
            "analysis_mode": "personalized",
            "selected_fields": request.selected_fields
        }
        
        app = create_report_analysis_workflow()
        result = await app.ainvoke(state)
        
        return ReportAnalyzeResponse(
            success=True,
            extracted_data=result.get("extracted_data"),
            file_name=request.file_name, 
            highlights=result.get("highlights"),
            detractors=result.get("detractors"),
            final_message=result.get("final_message"),
            metadata=result.get("metadata"),
            error=result.get("error")
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/extract", response_model=ReportAnalyzeResponse)
async def extract_report_data(request: ReportAnalyzeAutoRequest):
    """
    Apenas extra√ß√£o de dados (para UI de sele√ß√£o de campos).
    
    Retorna apenas os dados extra√≠dos do PDF, sem an√°lise ou formata√ß√£o.
    Usado para permitir que o usu√°rio selecione quais campos analisar.
    """
    try:
        state = {
            "file_content": request.file_content,
            "file_name": request.file_name,
            "user_id": request.user_id,
            "analysis_mode": "personalized",
            "selected_fields": None
        }
        
        app = create_report_analysis_workflow()
        result = await app.ainvoke(state)
        
        return ReportAnalyzeResponse(
            success=True,
            extracted_data=result.get("extracted_data"),
            file_name=request.file_name, 
            highlights=None,
            detractors=None,
            final_message=None,
            metadata=result.get("metadata"),
            error=result.get("error")
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/batch-analyze", response_model=BatchReportResponse)
async def batch_analyze_reports(request: BatchReportRequest):
    """
    Processa m√∫ltiplos relat√≥rios em paralelo (m√°ximo 10).
    
    Sempre usa modo 'auto' para batch processing.
    Cada arquivo √© processado independentemente.
    """
    try:
        if len(request.files) > 5:
            raise HTTPException(
                status_code=400, 
                detail="M√°ximo de 5 arquivos por lote"
            )
        
        results = await process_batch_reports(request.files, request.user_id)
        
        # Converter resultados para ReportAnalyzeResponse
        report_responses = []
        for result in results:
            if result.get("success"):
                # Sucesso: converter para ReportAnalyzeResponse
                report_responses.append(ReportAnalyzeResponse(
                    success=True,
                    extracted_data=result["data"].get("extracted_data"),
                    file_name=result.get("file_name"),
                    highlights=result["data"].get("highlights"),
                    detractors=result["data"].get("detractors"),
                    final_message=result["data"].get("final_message"),
                    metadata=result["data"].get("metadata"),
                    error=None
                ))
            else:
                # Falha: criar ReportAnalyzeResponse com erro
                report_responses.append(ReportAnalyzeResponse(
                    success=False,
                    file_name=result.get("file_name"),
                    extracted_data=None,
                    highlights=None,
                    detractors=None,
                    final_message=None,
                    metadata=None,
                    error=result.get("error", "Erro desconhecido")
                ))
        
        success_count = sum(1 for r in report_responses if r.success)
        
        return BatchReportResponse(
            success=True,
            results=report_responses,
            metadata={
                "total_files": len(request.files),
                "success_count": success_count,
                "failure_count": len(request.files) - success_count
            }
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

        # ============= ultra_batch (ultra lote) =============

# ============= GENERATE UPLOAD URLS =============

@router.post("/generate-upload-urls", response_model=GenerateUploadUrlsResponse)
async def generate_upload_urls(request: GenerateUploadUrlsRequest):
    """
    Gera Signed URLs para upload direto ao GCS.
    
    Frontend usa essas URLs para fazer upload paralelo dos arquivos,
    bypassando completamente o servidor Next.js.
    
    Fluxo:
    1. Frontend envia nomes dos arquivos
    2. Backend gera batch_id √∫nico
    3. Backend gera Signed URL para cada arquivo
    4. Backend salva metadados do batch no Firestore
    5. Backend retorna batch_id + Signed URLs
    """
    logger.info(f"üîó [SIGNED-URL] Request recebido: user_id={request.user_id}, chat_id={request.chat_id}, files={len(request.file_names)}")
    
    try:
        # 1. Gerar batch_id √∫nico
        batch_id = str(uuid.uuid4())
        logger.info(f"üîó [SIGNED-URL] Batch ID gerado: {batch_id}")
        
        # 2. Obter cliente Firestore
        db = get_firestore_client()
        
        # 3. Preparar lista de upload URLs
        upload_urls = []
        storage_paths = []
        
        # 4. Gerar Signed URL para cada arquivo
        for file_name in request.file_names:
            # Criar caminho no GCS: ultra-batch/{batch_id}/{fileName}
            storage_path = f"ultra-batch/{batch_id}/{file_name}"
            
            try:
                # Gerar Signed URL com permiss√£o WRITE (m√©todo PUT)
                signed_url = generate_signed_url_for_upload(
                    blob_path=storage_path,
                    expiration_hours=1  # URLs expiram em 1 hora
                )
                
                upload_urls.append({
                    "fileName": file_name,
                    "signedUrl": signed_url,
                    "storagePath": storage_path
                })
                storage_paths.append(storage_path)
                
                logger.info(f"‚úÖ [SIGNED-URL] URL gerada para: {file_name}")
                
            except Exception as url_error:
                logger.error(f"‚ùå [SIGNED-URL] Erro ao gerar URL para {file_name}: {url_error}")
                # N√£o falhar o request inteiro, apenas logar o erro
                continue
        
        # 5. Validar que pelo menos uma URL foi gerada
        if not upload_urls:
            logger.error("‚ùå [SIGNED-URL] Nenhuma URL foi gerada com sucesso")
            raise HTTPException(
                status_code=500,
                detail="N√£o foi poss√≠vel gerar Signed URLs. Verifique os logs."
            )
        
        # 6. Salvar metadados do batch no Firestore
        batch_ref = db.collection('ultra_batch_uploads').document(batch_id)
        batch_data = {
            'user_id': request.user_id,
            'file_names': request.file_names,
            'storage_paths': storage_paths,
            'status': 'uploading',  # Status inicial: aguardando upload
            'created_at': firestore.SERVER_TIMESTAMP,
            'total_files': len(request.file_names)
        }
        
        # Adicionar chat_id se fornecido
        if request.chat_id:
            batch_data['chat_id'] = request.chat_id
        
        batch_ref.set(batch_data)
        logger.info(f"‚úÖ [SIGNED-URL] Metadados do batch salvos no Firestore: {batch_id}")
        
        # 7. Retornar response
        return GenerateUploadUrlsResponse(
            batch_id=batch_id,
            upload_urls=upload_urls
        )
        
    except HTTPException:
        # Re-raise HTTP exceptions (j√° t√™m status code apropriado)
        raise
    except Exception as e:
        logger.error(f"‚ùå [SIGNED-URL] Erro cr√≠tico: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erro ao gerar Signed URLs: {str(e)}"
        )

# Implementar o endpoint (adicionar no final do arquivo):
@router.post("/ultra-batch-analyze", response_model=UltraBatchReportResponse)
async def ultra_batch_analyze_reports(request: UltraBatchReportRequest, background_tasks: BackgroundTasks):
    """
    An√°lise em ultra lote (at√© 100 relat√≥rios).
    
    Agora recebe apenas batch_id (arquivos j√° est√£o no GCS via upload direto).
    Busca metadados do batch no Firestore e processa os arquivos em background.
    """
    logger.info(f"üîó [ULTRA-BATCH] Request recebido: batch_id={request.batch_id}, user_id={request.user_id}, chat_id={request.chat_id}")

    try:
        # 1. Obter cliente Firestore
        db = get_firestore_client()
        
        # 2. Buscar metadados do batch no Firestore
        batch_ref = db.collection('ultra_batch_uploads').document(request.batch_id)
        batch_doc = batch_ref.get()
        
        if not batch_doc.exists:
            logger.error(f"‚ùå [ULTRA-BATCH] Batch n√£o encontrado: {request.batch_id}")
            raise HTTPException(
                status_code=404,
                detail=f"Batch {request.batch_id} n√£o encontrado. Verifique se o upload foi conclu√≠do."
            )
        
        batch_data = batch_doc.to_dict()
        total_files = batch_data.get('total_files', 0)
        storage_paths = batch_data.get('storage_paths', [])
        
        # 3. Validar que o batch pertence ao usu√°rio
        if batch_data.get('user_id') != request.user_id:
            logger.error(f"‚ùå [ULTRA-BATCH] Batch {request.batch_id} n√£o pertence ao usu√°rio {request.user_id}")
            raise HTTPException(
                status_code=403,
                detail="Voc√™ n√£o tem permiss√£o para processar este batch."
            )
        
        # 4. Gerar job_id √∫nico para o processamento
        job_id = str(uuid.uuid4())
        logger.info(f"üîó [ULTRA-BATCH] Job ID gerado: {job_id}")
        
        # 5. Salvar job no Firestore
        job_ref = db.collection('ultra_batch_jobs').document(job_id)
        job_data = {
            'user_id': request.user_id,
            'batch_id': request.batch_id,  # Vincular job ao batch
            'total_files': total_files,
            'status': 'processing',
            'current_file': 0,
            'created_at': firestore.SERVER_TIMESTAMP,
            'estimated_time_minutes': total_files * 2  # 2 min por arquivo
        }
        
        # Se chat_id foi fornecido, salv√°-lo no job
        if request.chat_id:
            job_data['chat_id'] = request.chat_id
        
        job_ref.set(job_data)
        logger.info(f"‚úÖ [ULTRA-BATCH] Job criado no Firestore: {job_id}")
        
        # 6. Atualizar status do batch para 'processing'
        batch_ref.update({
            'status': 'processing',
            'job_id': job_id
        })
        
        # 7. üîó PADR√ÉO DE PONTEIRO: Se chat_id foi fornecido, salvar batchJobId no documento do chat
        if request.chat_id:
            try:
                chat_ref = db.collection('users').document(request.user_id).collection('chats').document(request.chat_id)
                chat_ref.update({
                    'batchJobId': job_id,
                    'statusJob': 'processing'
                })
                logger.info(f"‚úÖ [ULTRA-BATCH] Ponteiro salvo: chat {request.chat_id} -> job {job_id}")
            except Exception as e:
                # N√£o falhar o job se n√£o conseguir atualizar o chat
                logger.warning(f"‚ö†Ô∏è [ULTRA-BATCH] Erro ao salvar ponteiro no chat {request.chat_id}: {e}")
        
        # 8. Iniciar processamento em background
        # Passar batch_id em vez de files (arquivos j√° est√£o no GCS)
        background_tasks.add_task(
            process_ultra_batch_reports,
            request.batch_id,  # ‚Üê MUDAN√áA: passar batch_id em vez de files
            request.user_id,
            job_id
        )
        
        logger.info(f"‚úÖ [ULTRA-BATCH] Processamento iniciado em background para job {job_id}")
        
        return UltraBatchReportResponse(
            success=True,
            job_id=job_id,
            total_files=total_files,
            estimated_time_minutes=total_files * 2
        )
        
    except HTTPException:
        # Re-raise HTTP exceptions (j√° t√™m status code apropriado)
        raise
    except Exception as e:
        logger.error(f"‚ùå [ULTRA-BATCH] Erro cr√≠tico: {e}")
        return UltraBatchReportResponse(
            success=False,
            job_id="",
            total_files=0,
            estimated_time_minutes=0,
            error=str(e)
        )



@router.get("/ultra-batch-status/{job_id}")
async def get_ultra_batch_status(job_id: str):
    """
    Consulta status e resultados do job ultra batch.
    
    Retorna dados em tempo real do Firestore:
    - Status do job (processing/completed/failed)
    - Progresso atual
    - Resultados j√° processados (exposi√ß√£o incremental)
    """
    try:
        db = get_firestore_client()
        job_ref = db.collection('ultra_batch_jobs').document(job_id)
        
        # Buscar dados do job
        job_doc = job_ref.get()
        if not job_doc.exists:
            raise HTTPException(status_code=404, detail="Job n√£o encontrado")
        
        job_data = job_doc.to_dict()
        
        # Buscar resultados j√° processados
        results_ref = job_ref.collection('results')
        results_docs = results_ref.order_by('processedAt').stream()
        
        results = []
        for doc in results_docs:
            result_data = doc.to_dict()
            results.append({
                "fileIndex": int(doc.id),
                "fileName": result_data.get("fileName"),
                "success": result_data.get("success", False),
                "finalMessage": result_data.get("final_message"),
                "error": result_data.get("error"),
                "processedAt": result_data.get("processedAt")
            })
        
        return {
            "success": True,
            "jobId": job_id,
            "status": job_data.get("status", "unknown"),
            "progress": {
                "processedFiles": job_data.get("processedFiles", 0),
                "totalFiles": job_data.get("total_files", 0),
                "successCount": job_data.get("successCount", 0),
                "failureCount": job_data.get("failureCount", 0)
            },
            "results": results,
            "createdAt": job_data.get("created_at"),
            "completedAt": job_data.get("completedAt"),
            "estimatedTimeMinutes": job_data.get("estimated_time_minutes", 0)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erro ao consultar job: {str(e)}")



# ============= STREAMING ENDPOINTS (SSE) =============

@router.post("/analyze-auto-stream")
async def analyze_report_auto_stream(request: ReportAnalyzeAutoRequest):
    """
    An√°lise autom√°tica com streaming de progresso (SSE).
    
    Retorna progresso em tempo real via Server-Sent Events.
    """
    async def event_generator():
        async def progress_callback(status):
            yield f"data: {json.dumps(status)}\n\n"
        
        state = {
            "file_content": request.file_content,
            "file_name": request.file_name,
            "user_id": request.user_id,
            "analysis_mode": "auto",
            "selected_fields": None
        }
        
        # TODO: Implementar analyze_report_with_progress
        # Por enquanto, usar o workflow normal
        app = create_report_analysis_workflow()
        result = await app.ainvoke(state)
        
        # Enviar resultado final
        yield f"data: {json.dumps({'done': True, 'result': result})}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )


@router.post("/analyze-personalized-stream")
async def analyze_report_personalized_stream(request: ReportAnalyzePersonalizedRequest):
    """
    An√°lise personalizada com streaming de progresso (SSE).
    
    Retorna progresso em tempo real via Server-Sent Events.
    """
    async def event_generator():
        async def progress_callback(status):
            yield f"data: {json.dumps(status)}\n\n"
        
        state = {
            "file_content": request.file_content,
            "file_name": request.file_name,
            "user_id": request.user_id,
            "analysis_mode": "personalized",
            "selected_fields": request.selected_fields
        }
        
        # TODO: Implementar analyze_report_with_progress
        # Por enquanto, usar o workflow normal
        result = await report_analysis_app.ainvoke(state)
        
        # Enviar resultado final
        yield f"data: {json.dumps({'done': True, 'result': result})}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )

class PersonalizedAnalysisRequest(BaseModel):
    extracted_data: Dict[str, Any]
    selected_fields: Dict[str, Any]
    file_name: str
    user_id: str

# Substituir linhas 270-311 por:
@router.post("/analyze-personalized-from-data")
async def analyze_personalized_from_data(
    request: PersonalizedAnalysisRequest
):
    """
    An√°lise personalizada usando dados j√° extra√≠dos.
    """
    try:
        print(f"[analyze_personalized_from_data] Iniciando an√°lise com dados j√° extra√≠dos")
        print(f"[analyze_personalized_from_data] Campos selecionados: {len(request.selected_fields)}")

        state = {
            "extracted_data": request.extracted_data,
            "file_name": request.file_name,
            "user_id": request.user_id,
            "analysis_mode": "personalized",
            "selected_fields": request.selected_fields
        }
        
        # Pular extra√ß√£o e ir direto para an√°lise
        app = create_report_analysis_workflow_from_data()
        result = await app.ainvoke(state)

        print(f"[analyze_personalized_from_data] ‚úÖ An√°lise conclu√≠da")
        
        return ReportAnalyzeResponse(
            success=True,
            extracted_data=result.get("extracted_data"),
            file_name=request.file_name, 
            highlights=result.get("highlights"),
            detractors=result.get("detractors"),
            final_message=result.get("final_message"),
            metadata=result.get("metadata"),
            error=result.get("error")
        )
    except Exception as e:
        print(f"[analyze_personalized_from_data] ‚ùå Erro: {e}")
        raise HTTPException(status_code=500, detail=str(e))