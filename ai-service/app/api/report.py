"""
Endpoints para anÃ¡lise de relatÃ³rios XP.
"""
from typing import Dict, Any, List
import json
from fastapi import APIRouter, HTTPException, BackgroundTasks, Request, Query
from fastapi.responses import StreamingResponse

from app.models.requests import (    
    ReportAnalyzeAutoRequest, 
    ReportAnalyzePersonalizedRequest,
    ReportAnalyzeResponse,
    BatchReportRequest,
    BatchReportResponse,
    UltraBatchReportRequest, 
    UltraBatchReportResponse,
    GenerateUploadUrlsRequest,
    GenerateUploadUrlsResponse,
    MetricsSummaryResponse,
    MetricsSummaryItem,
)
from app.workflows.report_workflow import create_report_analysis_workflow, create_report_analysis_workflow_from_data

from app.services.report_analyzer.batch_processing import process_batch_reports
from app.services.report_analyzer.ultra_batch_processing import process_ultra_batch_reports  
from app.services.report_analyzer.signed_url import generate_signed_url_for_upload

from pydantic import BaseModel

from app.config import get_firestore_client, logger
from firebase_admin import firestore  
import uuid
from app.services.metrics import record_metric_call, record_ultra_batch_start, record_ultra_batch_complete 



router = APIRouter(tags=["report"])  # Remover o prefix


@router.post("/analyze-auto", response_model=ReportAnalyzeResponse)
async def analyze_report_auto(request: ReportAnalyzeAutoRequest):
    """
    AnÃ¡lise automÃ¡tica completa (todos os dados).
    
    Processa o relatÃ³rio XP e retorna:
    - Dados extraÃ­dos
    - AnÃ¡lise de performance
    - Highlights e detractors
    - Mensagem WhatsApp formatada
    """
    try:
        state = {
            "file_content": request.file_content,
            "file_name": request.file_name,
            "user_id": request.user_id,
            "analysis_mode": "auto",
            "selected_fields": None
        }
        
        app = create_report_analysis_workflow()
        result = await app.ainvoke(state)
        
        # Registrar mÃ©trica apÃ³s processamento bem-sucedido
        if result.get("error") is None:
            try:
                record_metric_call(request.user_id, "automatica")
            except Exception as e:
                logger.error(f"Erro ao registrar mÃ©trica analise_automatica: {e}")
        
        return ReportAnalyzeResponse(
            success=True,
            extracted_data=result.get("extracted_data"),
            file_name=request.file_name, 
            highlights=result.get("highlights"),
            detractors=result.get("detractors"),
            final_message=result.get("final_message"),
            metadata=result.get("metadata"),
            error=result.get("error")
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/analyze-personalized", response_model=ReportAnalyzeResponse)
async def analyze_report_personalized(request: ReportAnalyzePersonalizedRequest):
    """
    AnÃ¡lise personalizada com campos selecionados pelo usuÃ¡rio.
    
    Processa apenas os campos escolhidos pelo usuÃ¡rio:
    - Campos top-level (monthlyReturn, yearlyReturn, etc.)
    - Classes de ativo especÃ­ficas
    - Highlights/detractors filtrados
    """
    try:
        state = {
            "file_content": request.file_content,
            "file_name": request.file_name,
            "user_id": request.user_id,
            "analysis_mode": "personalized",
            "selected_fields": request.selected_fields
        }
        
        app = create_report_analysis_workflow()
        result = await app.ainvoke(state)
        
        # Registrar mÃ©trica apÃ³s processamento bem-sucedido
        if result.get("error") is None:
            try:
                record_metric_call(request.user_id, "personalized")
            except Exception as e:
                logger.error(f"Erro ao registrar mÃ©trica analise_personalized: {e}")
        
        return ReportAnalyzeResponse(
            success=True,
            extracted_data=result.get("extracted_data"),
            file_name=request.file_name, 
            highlights=result.get("highlights"),
            detractors=result.get("detractors"),
            final_message=result.get("final_message"),
            metadata=result.get("metadata"),
            error=result.get("error")
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/extract", response_model=ReportAnalyzeResponse)
async def extract_report_data(request: ReportAnalyzeAutoRequest):
    """
    Apenas extraÃ§Ã£o de dados (para UI de seleÃ§Ã£o de campos).
    
    Retorna apenas os dados extraÃ­dos do PDF, sem anÃ¡lise ou formataÃ§Ã£o.
    Usado para permitir que o usuÃ¡rio selecione quais campos analisar.
    """
    try:
        state = {
            "file_content": request.file_content,
            "file_name": request.file_name,
            "user_id": request.user_id,
            "analysis_mode": "extract_only",
            "selected_fields": None
        }
        
        app = create_report_analysis_workflow()
        result = await app.ainvoke(state)
        
        return ReportAnalyzeResponse(
            success=True,
            extracted_data=result.get("extracted_data"),
            file_name=request.file_name, 
            highlights=None,
            detractors=None,
            final_message=None,
            metadata=result.get("metadata"),
            error=result.get("error")
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/batch-analyze", response_model=BatchReportResponse)
async def batch_analyze_reports(request: BatchReportRequest):
    """
    Processa mÃºltiplos relatÃ³rios em paralelo (mÃ¡ximo 10).
    
    Sempre usa modo 'auto' para batch processing.
    Cada arquivo Ã© processado independentemente.
    """
    try:
        if len(request.files) > 5:
            raise HTTPException(
                status_code=400, 
                detail="MÃ¡ximo de 5 arquivos por lote"
            )
        
        results = await process_batch_reports(request.files, request.user_id)

        success_count = sum(1 for r in results if r.get("success"))
        for _ in range(success_count):
            try:
                record_metric_call(request.user_id, "automatica")
            except Exception as e:
                logger.error(f"Erro ao registrar mÃ©trica analise_automatica: {e}")
        
        # Converter resultados para ReportAnalyzeResponse
        report_responses = []
        for result in results:
            if result.get("success"):
                # Sucesso: converter para ReportAnalyzeResponse
                report_responses.append(ReportAnalyzeResponse(
                    success=True,
                    extracted_data=result["data"].get("extracted_data"),
                    file_name=result.get("file_name"),
                    highlights=result["data"].get("highlights"),
                    detractors=result["data"].get("detractors"),
                    final_message=result["data"].get("final_message"),
                    metadata=result["data"].get("metadata"),
                    error=None
                ))
            else:
                # Falha: criar ReportAnalyzeResponse com erro
                report_responses.append(ReportAnalyzeResponse(
                    success=False,
                    file_name=result.get("file_name"),
                    extracted_data=None,
                    highlights=None,
                    detractors=None,
                    final_message=None,
                    metadata=None,
                    error=result.get("error", "Erro desconhecido")
                ))
        
        success_count = sum(1 for r in report_responses if r.success)
        
        return BatchReportResponse(
            success=True,
            results=report_responses,
            metadata={
                "total_files": len(request.files),
                "success_count": success_count,
                "failure_count": len(request.files) - success_count
            }
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============= GENERATE UPLOAD URLS =============

@router.post("/generate-upload-urls", response_model=GenerateUploadUrlsResponse)
async def generate_upload_urls(request: GenerateUploadUrlsRequest):
    """
    Gera Signed URLs para upload direto ao GCS.
    
    Frontend usa essas URLs para fazer upload paralelo dos arquivos,
    bypassando completamente o servidor Next.js.
    
    Fluxo:
    1. Frontend envia nomes dos arquivos
    2. Backend gera batch_id Ãºnico
    3. Backend gera Signed URL para cada arquivo
    4. Backend salva metadados do batch no Firestore
    5. Backend retorna batch_id + Signed URLs
    """
    logger.info(f"ğŸ”— [SIGNED-URL] Request recebido: user_id={request.user_id}, chat_id={request.chat_id}, files={len(request.file_names)}")
    
    try:
        # 1. Gerar batch_id Ãºnico
        batch_id = str(uuid.uuid4())
        logger.info(f"ğŸ”— [SIGNED-URL] Batch ID gerado: {batch_id}")
        
        # 2. Obter cliente Firestore
        db = get_firestore_client()
        
        # 3. Preparar lista de upload URLs
        upload_urls = []
        storage_paths = []
        
        # 4. Gerar Signed URL para cada arquivo
        for file_name in request.file_names:
            # Criar caminho no GCS: ultra-batch/{batch_id}/{fileName}
            storage_path = f"ultra-batch/{batch_id}/{file_name}"
            
            try:
                # Gerar Signed URL com permissÃ£o WRITE (mÃ©todo PUT)
                signed_url = generate_signed_url_for_upload(
                    blob_path=storage_path,
                    expiration_hours=1  # URLs expiram em 1 hora
                )
                
                upload_urls.append({
                    "fileName": file_name,
                    "signedUrl": signed_url,
                    "storagePath": storage_path
                })
                storage_paths.append(storage_path)
                
                logger.info(f"âœ… [SIGNED-URL] URL gerada para: {file_name}")
                
            except Exception as url_error:
                logger.error(f"âŒ [SIGNED-URL] Erro ao gerar URL para {file_name}: {url_error}")
                # NÃ£o falhar o request inteiro, apenas logar o erro
                continue
        
        # 5. Validar que pelo menos uma URL foi gerada
        if not upload_urls:
            logger.error("âŒ [SIGNED-URL] Nenhuma URL foi gerada com sucesso")
            raise HTTPException(
                status_code=500,
                detail="NÃ£o foi possÃ­vel gerar Signed URLs. Verifique os logs."
            )
        
        # 6. Salvar metadados do batch no Firestore
        batch_ref = db.collection('ultra_batch_uploads').document(batch_id)
        batch_data = {
            'user_id': request.user_id,
            'file_names': request.file_names,
            'storage_paths': storage_paths,
            'status': 'uploading',  # Status inicial: aguardando upload
            'created_at': firestore.SERVER_TIMESTAMP,
            'total_files': len(request.file_names)
        }
        
        # Adicionar chat_id se fornecido
        if request.chat_id:
            batch_data['chat_id'] = request.chat_id
        
        batch_ref.set(batch_data)
        logger.info(f"âœ… [SIGNED-URL] Metadados do batch salvos no Firestore: {batch_id}")
        
        # 7. Retornar response
        return GenerateUploadUrlsResponse(
            batch_id=batch_id,
            upload_urls=upload_urls
        )
        
    except HTTPException:
        # Re-raise HTTP exceptions (jÃ¡ tÃªm status code apropriado)
        raise
    except Exception as e:
        logger.error(f"âŒ [SIGNED-URL] Erro crÃ­tico: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erro ao gerar Signed URLs: {str(e)}"
        )

# Implementar o endpoint (adicionar no final do arquivo):
@router.post("/ultra-batch-analyze")
async def ultra_batch_analyze_reports(request: Request, body: UltraBatchReportRequest):
    """
    AnÃ¡lise em ultra lote (atÃ© 100 relatÃ³rios) com Streaming Response (SSE).
    
    Agora recebe apenas batch_id (arquivos jÃ¡ estÃ£o no GCS via upload direto).
    MantÃ©m conexÃ£o HTTP aberta enviando eventos de progresso.
    """
    logger.info(f"ğŸ”— [ULTRA-BATCH] Request recebido: batch_id={body.batch_id}, user_id={body.user_id}, chat_id={body.chat_id}")

    try:
        # 1. Obter cliente Firestore
        db = get_firestore_client()
        
        # 2. Buscar metadados do batch no Firestore
        batch_ref = db.collection('ultra_batch_uploads').document(body.batch_id)
        batch_doc = batch_ref.get()
        
        if not batch_doc.exists:
            logger.error(f"âŒ [ULTRA-BATCH] Batch nÃ£o encontrado: {body.batch_id}")
            raise HTTPException(
                status_code=404,
                detail=f"Batch {body.batch_id} nÃ£o encontrado. Verifique se o upload foi concluÃ­do."
            )
        
        batch_data = batch_doc.to_dict()
        total_files = batch_data.get('total_files', 0)
        
        # 3. Validar que o batch pertence ao usuÃ¡rio
        if batch_data.get('user_id') != body.user_id:
            logger.error(f"âŒ [ULTRA-BATCH] Batch {body.batch_id} nÃ£o pertence ao usuÃ¡rio {body.user_id}")
            raise HTTPException(
                status_code=403,
                detail="VocÃª nÃ£o tem permissÃ£o para processar este batch."
            )
        
        # 4. Gerar job_id Ãºnico para o processamento
        job_id = str(uuid.uuid4())
        logger.info(f"ğŸ”— [ULTRA-BATCH] Job ID gerado: {job_id}")
        
        # 5. Salvar job no Firestore
        job_ref = db.collection('ultra_batch_jobs').document(job_id)
        job_data = {
            'user_id': body.user_id,
            'batch_id': body.batch_id,  # Vincular job ao batch
            'total_files': total_files,
            'status': 'processing',
            'current_file': 0,
            'created_at': firestore.SERVER_TIMESTAMP,
            'estimated_time_minutes': total_files * 2  # 2 min por arquivo
        }
        
        # Se chat_id foi fornecido, salvÃ¡-lo no job
        if body.chat_id:
            job_data['chat_id'] = body.chat_id
        
        job_ref.set(job_data)
        logger.info(f"âœ… [ULTRA-BATCH] Job criado no Firestore: {job_id}")
        
        # Registrar mÃ©trica de ultra-batch apÃ³s criar job
        try:
            record_ultra_batch_start(body.user_id, job_id, total_files)
        except Exception as e:
            logger.error(f"Erro ao registrar mÃ©trica ultra-batch: {e}")
        
        # 6. Atualizar status do batch para 'processing'
        batch_ref.update({
            'status': 'processing',
            'job_id': job_id
        })
        
        # 7. ğŸ”— PADRÃƒO DE PONTEIRO: Se chat_id foi fornecido, salvar batchJobId no documento do chat
        if body.chat_id:
            try:
                chat_ref = db.collection('users').document(body.user_id).collection('chats').document(body.chat_id)
                chat_ref.update({
                    'batchJobId': job_id,
                    'statusJob': 'processing'
                })
                logger.info(f"âœ… [ULTRA-BATCH] Ponteiro salvo: chat {body.chat_id} -> job {job_id}")
            except Exception as e:
                # NÃ£o falhar o job se nÃ£o conseguir atualizar o chat
                logger.warning(f"âš ï¸ [ULTRA-BATCH] Erro ao salvar ponteiro no chat {body.chat_id}: {e}")
        
        
        logger.info(f"âœ… [ULTRA-BATCH] Iniciando stream para job {job_id}")
        
        async def stream_wrapper():
            try:
                # Consumir o generator do serviÃ§o
                async for event in process_ultra_batch_reports(body.batch_id, body.user_id, job_id):
                    # Verificar desconexÃ£o (apenas logar, nÃ£o cancelar para permitir refresh)
                    if await request.is_disconnected():
                        logger.warning(f"âš ï¸ [ULTRA-BATCH] Cliente desconectou do job {job_id} (mantendo processamento)")
                        # NÃ£o cancelar o job aqui. O Cloud Run deve manter a instÃ¢ncia viva pelo tempo que puder.
                        # Se quisermos cancelar explicitamente, deve ser via outro endpoint ou lÃ³gica de timeout.
                    
                    yield event
            except Exception as e:
                logger.error(f"âŒ [ULTRA-BATCH] Erro no stream wrapper: {e}")
                yield f"data: {json.dumps({'event': 'fatal_error', 'error': str(e)})}\n\n"

        return StreamingResponse(
            stream_wrapper(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )
        
    except HTTPException:
        # Re-raise HTTP exceptions (jÃ¡ tÃªm status code apropriado)
        raise
    except Exception as e:
        logger.error(f"âŒ [ULTRA-BATCH] Erro crÃ­tico: {e}")
        raise HTTPException(status_code=500, detail=str(e))



@router.get("/ultra-batch-status/{job_id}")
async def get_ultra_batch_status(job_id: str):
    """
    Consulta status e resultados do job ultra batch.
    
    Retorna dados em tempo real do Firestore:
    - Status do job (processing/completed/failed)
    - Progresso atual
    - Resultados jÃ¡ processados (exposiÃ§Ã£o incremental)
    """
    try:
        db = get_firestore_client()
        job_ref = db.collection('ultra_batch_jobs').document(job_id)
        
        # Buscar dados do job
        job_doc = job_ref.get()
        if not job_doc.exists:
            raise HTTPException(status_code=404, detail="Job nÃ£o encontrado")
        
        job_data = job_doc.to_dict()
        
        # Buscar resultados jÃ¡ processados
        results_ref = job_ref.collection('results')
        results_docs = results_ref.order_by('processedAt').stream()
        
        results = []
        for doc in results_docs:
            result_data = doc.to_dict()
            results.append({
                "fileIndex": int(doc.id),
                "fileName": result_data.get("fileName"),
                "success": result_data.get("success", False),
                "finalMessage": result_data.get("final_message"),
                "error": result_data.get("error"),
                "processedAt": result_data.get("processedAt")
            })
        
        return {
            "success": True,
            "jobId": job_id,
            "status": job_data.get("status", "unknown"),
            "progress": {
                "processedFiles": job_data.get("processedFiles", 0),
                "totalFiles": job_data.get("total_files", 0),
                "successCount": job_data.get("successCount", 0),
                "failureCount": job_data.get("failureCount", 0)
            },
            "results": results,
            "createdAt": job_data.get("created_at"),
            "completedAt": job_data.get("completedAt"),
            "estimatedTimeMinutes": job_data.get("estimated_time_minutes", 0)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erro ao consultar job: {str(e)}")


def _month_range(from_month: str, to_month: str, max_months: int = 24) -> list[str]:
    from datetime import datetime
    f = datetime.strptime(from_month, "%Y-%m")
    t = datetime.strptime(to_month, "%Y-%m")
    if f > t:
        return []
    out: list[str] = []
    y, m = f.year, f.month
    ty, tm = t.year, t.month
    while (y, m) <= (ty, tm) and len(out) < max_months:
        out.append(f"{y}-{m:02d}")
        if m == 12:
            y, m = y + 1, 1
        else:
            m += 1
    return out


@router.get("/metrics-summary", response_model=MetricsSummaryResponse)
async def get_metrics_summary(
    from_month: str = Query(..., description="MÃªs inicial YYYY-MM"),
    to_month: str = Query(..., description="MÃªs final YYYY-MM"),
):
    """
    Retorna resumos mensais de mÃ©tricas do report_analyzer (MAU, volume, intensidade, qualidade, escala).
    Dados prÃ©-agregados pelo job Cloud Scheduler; intervalo limitado a 24 meses.
    """
    try:
        months = _month_range(from_month, to_month)
        if not months:
            raise HTTPException(
                status_code=400,
                detail="from_month deve ser anterior ou igual a to_month",
            )
        db = get_firestore_client()
        summaries: List[MetricsSummaryItem] = []
        for month in months:
            ref = db.collection("metrics_summary").document(month)
            doc = ref.get()
            if doc.exists:
                data = doc.to_dict() or {}
                data["month"] = month
                if "updated_at" in data and hasattr(data["updated_at"], "isoformat"):
                    data["updated_at"] = data["updated_at"].isoformat()
                summaries.append(MetricsSummaryItem(**data))
        return MetricsSummaryResponse(summaries=summaries)
    except HTTPException:
        raise
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.exception("Erro ao obter metrics-summary: %s", e)
        raise HTTPException(status_code=500, detail=str(e))


# ============= STREAMING ENDPOINTS (SSE) =============

@router.post("/analyze-auto-stream")
async def analyze_report_auto_stream(request: ReportAnalyzeAutoRequest):
    """
    AnÃ¡lise automÃ¡tica com streaming de progresso (SSE).
    
    Retorna progresso em tempo real via Server-Sent Events.
    """
    async def event_generator():
        async def progress_callback(status):
            yield f"data: {json.dumps(status)}\n\n"
        
        state = {
            "file_content": request.file_content,
            "file_name": request.file_name,
            "user_id": request.user_id,
            "analysis_mode": "auto",
            "selected_fields": None
        }
        
        # TODO: Implementar analyze_report_with_progress
        # Por enquanto, usar o workflow normal
        app = create_report_analysis_workflow()
        result = await app.ainvoke(state)

        if result.get("error") is None:
            try:
                record_metric_call(request.user_id, "automatica")
            except Exception as e:
                logger.error(f"Erro ao registrar mÃ©trica analise_automatica: {e}")

        yield f"data: {json.dumps({'done': True, 'result': result})}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )


@router.post("/analyze-personalized-stream")
async def analyze_report_personalized_stream(request: ReportAnalyzePersonalizedRequest):
    """
    AnÃ¡lise personalizada com streaming de progresso (SSE).
    
    Retorna progresso em tempo real via Server-Sent Events.
    """
    async def event_generator():
        async def progress_callback(status):
            yield f"data: {json.dumps(status)}\n\n"
        
        state = {
            "file_content": request.file_content,
            "file_name": request.file_name,
            "user_id": request.user_id,
            "analysis_mode": "personalized",
            "selected_fields": request.selected_fields
        }
        
        # TODO: Implementar analyze_report_with_progress
        # Por enquanto, usar o workflow normal
        app = create_report_analysis_workflow()
        result = await app.ainvoke(state)

        if result.get("error") is None:
            try:
                record_metric_call(request.user_id, "personalized")
            except Exception as e:
                logger.error(f"Erro ao registrar mÃ©trica analise_personalized: {e}")

        yield f"data: {json.dumps({'done': True, 'result': result})}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )

class PersonalizedAnalysisRequest(BaseModel):
    extracted_data: Dict[str, Any]
    selected_fields: Dict[str, Any]
    file_name: str
    user_id: str

@router.post("/analyze-personalized-from-data")
async def analyze_personalized_from_data(
    request: PersonalizedAnalysisRequest
):
    """
    AnÃ¡lise personalizada usando dados jÃ¡ extraÃ­dos.
    """
    try:
        logger.info("[analyze_personalized_from_data] Iniciando anÃ¡lise com dados jÃ¡ extraÃ­dos")
        logger.debug(
            "[analyze_personalized_from_data] Campos selecionados: %d",
            len(request.selected_fields),
        )

        state = {
            "extracted_data": request.extracted_data,
            "file_name": request.file_name,
            "user_id": request.user_id,
            "analysis_mode": "personalized",
            "selected_fields": request.selected_fields
        }
        
        # Pular extraÃ§Ã£o e ir direto para anÃ¡lise
        app = create_report_analysis_workflow_from_data()
        result = await app.ainvoke(state)

        if result.get("error") is None:
            try:
                record_metric_call(request.user_id, "personalized")
            except Exception as e:
                logger.error(f"Erro ao registrar mÃ©trica analise_personalized: {e}")

        logger.info("[analyze_personalized_from_data] AnÃ¡lise concluÃ­da")

        return ReportAnalyzeResponse(
            success=True,
            extracted_data=result.get("extracted_data"),
            file_name=request.file_name, 
            highlights=result.get("highlights"),
            detractors=result.get("detractors"),
            final_message=result.get("final_message"),
            metadata=result.get("metadata"),
            error=result.get("error")
        )
    except Exception as e:
        logger.error("[analyze_personalized_from_data] Erro: %s", e, exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
